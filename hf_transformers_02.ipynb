{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hf_transformers_02.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNUK3yLGxU7Jllw7ue+j15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hansglick/book_errata/blob/main/hf_transformers_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIMiAcf1j-hX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "kr8LhJsckM3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fonctionnement des transformers\n",
        "\n",
        "### Historique\n",
        "\n",
        " * 2017 : le papier *attention is all you need* qui présente pour la première fois l'architecture qu'on appelle les transformers. Le papier se focus sur la traduction avec une partie encoder et une partie decoder\n",
        " * 2018 : **GPT**, pretrained et finetunable sur diverses tâches NLP\n",
        " * 2018 : **BERT**, the model transformer par excellence, pré-entraîne et finetunable sur la plupart des tâches NLP avec un accent mis sur le résumé\n",
        " * 2019 : **GPT-2**, version encore plus grande que la première\n",
        " * 2019 : **DistillBert**, version allégée de BERT et presque tout aussi performante\n",
        " * 2020 : **BART** et **T5** deux modèles pretrained à la façon du 1er transformer (avec encoder et decoder). Ils sont également finetunable. Ce sont les premiers à faire ça\n",
        " * 2020 : **GPT-3**, modèle très grand, parfait car ne néccesitant pas de finetunning, le fameux zero shot learning\n",
        "\n",
        "### Grandes familles\n",
        "\n",
        "On distingue 3 grandes familles de transformers :\n",
        " * Les modèles autorégressifs comme GPT\n",
        " * Les modèles autoencodeurs comme BERT\n",
        " * Les modèles sequences2séquences comme T5 et BART"
      ],
      "metadata": {
        "id": "AJMnxXrGkOoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Terminologie\n",
        " * **Architecture** : les différents tuyaux qui forment le modèle. Le squelette du modèle. Voir la vidéo ici : https://www.youtube.com/watch?v=H39Z_720T5s&ab_channel=HuggingFace\n",
        " * **Checkpoints** : les poids du modèle qu'on va coller à l'architecture"
      ],
      "metadata": {
        "id": "6XSzS1x8mWHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encodeurs\n",
        "\n",
        "Souvent ils sont entraînés en tentant de prédire des mots masqués aléatoirement dans les phrases. Les plus fameux encoders sont : BERT, ALBERT, DISTILLBERT, ELECTRA, ROBERTA, voir la vidéo ici : https://www.youtube.com/watch?v=MUqNwgPjJvQ&ab_channel=HuggingFace"
      ],
      "metadata": {
        "id": "X3WZAGUCz1yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decodeurs\n",
        "\n",
        "Ce sont des modèles autorégressifs, un peu à la manière de RNN classique unidirectionnels. Ils cherchent à prédire les mots en se basant sur les mots précédents. Parfaitement adapté à de la génération de texte. Les plus fameux décodeurs sont : CTRL, TransformerXL, la famille des GPT, voir la vidéo ici : https://www.youtube.com/watch?v=d_ixlCubqQw\n"
      ],
      "metadata": {
        "id": "y4p-9n6I0eyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence2Sequence\n",
        "Particulièrement adapté aux tâches qui à partir d'une séquence donnée doivent générer une séquence de sortie, donc les tâches comme la traduction, le résumé, les questions réponses, etc. Les modèles les plus fameux sont : BART, mBART, Marian, T5. Voir la vidéo ici : https://www.youtube.com/watch?v=0_4KEb08xrE&ab_channel=HuggingFace"
      ],
      "metadata": {
        "id": "rQZShk4K1Fem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mx6H_kXP_Ros"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}