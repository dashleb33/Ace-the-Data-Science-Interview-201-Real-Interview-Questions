{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p009_Preprocessing_Layer_NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOrVCDUj3fuc9upHa8plcG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hansglick/book_errata/blob/main/p009_Preprocessing_Layer_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqRfawEnxhgV",
        "outputId": "d14b9c07-dc08-4499-8e84-87c9cd42cd1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# objectif\n",
        "\n",
        "Jouer avec les layers de preprocessing : \n",
        " * **TASK 1** : simuler données textuelles de training, tokenizer les mots, entrainer RNN dessus, simuler données de test, évaluer le modele\n",
        " * **TASK 2** : pareil sauf qu'on tokenize les ngrams\n",
        " * **TASK 3** : pareil sauf qu'on tonkenize en pondérant avec TFIDF rien compris"
      ],
      "metadata": {
        "id": "O5iiJlx5yGc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 1"
      ],
      "metadata": {
        "id": "6xX_PK5iVOIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulation des données\n",
        "sentences = tf.constant([\"The Sky is the limit my man\",\n",
        "                         \"I mean, I'm the best bruh\",\n",
        "                         \"I like the ragga dance hall so mi go so then\",\n",
        "                         \"You know bro, that's the life\"])"
      ],
      "metadata": {
        "id": "Y76dH6zFzSTq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize ce foutu texte bro\n",
        "vectorizer = tf.keras.layers.TextVectorization(output_mode = \"int\")\n",
        "vectorizer.adapt(sentences)"
      ],
      "metadata": {
        "id": "Pix9ixeJTo_K"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple\n",
        "vectorizer(tf.constant([\"Touch the sky\"])).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMrPvKK-cdLG",
        "outputId": "0c27bbc9-223d-45a4-d7d9-07cacaa1aff9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 8]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer un petit modèle avec API fonctionnelle :\n",
        "# Embedding layer de 16 dimensions\n",
        "# GRU à 8 units\n",
        "# Un layer dense qui renvoie du logit\n",
        "\n",
        "embedding_size = 16\n",
        "gru_units = 8\n",
        "input_layer = tf.keras.layers.Input(shape=(None,),dtype=\"int64\")\n",
        "x = tf.keras.layers.Embedding(input_dim=vectorizer.vocabulary_size(),output_dim=16)(input_layer)\n",
        "x = tf.keras.layers.GRU(units=8)(x)\n",
        "output = tf.keras.layers.Dense(units=1,activation=\"sigmoid\")(x)\n",
        "model = tf.keras.Model(input_layer,output)"
      ],
      "metadata": {
        "id": "Gr0AwZ9XTzV1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer un dataset de phrase de training\n",
        "# Vectorizer le comme il se doit avec le vectorizer\n",
        "training_sentences = ([\"I hate a salad yesterday\",\"My favorite color is purple\"],[1,0])\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices(training_sentences)\n",
        "training_dataset = training_dataset.map(lambda a,b : (vectorizer(a), b))\n",
        "training_dataset = training_dataset.batch(2)"
      ],
      "metadata": {
        "id": "o3vTnuFcUQ_i"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiler et trainer le modele\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy())\n",
        "model.fit(training_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JswA3HAOUq_d",
        "outputId": "17656d13-eb47-422f-d07a-191d08b39cae"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step - loss: 0.6969\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fafcbf348d0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour l'inférence, il faut modifier le modele existant pour qu'il accepte les string as input\n",
        "input_string = tf.keras.layers.Input(shape=(1,),dtype=\"string\")\n",
        "x = vectorizer(input_string)\n",
        "new_output = model(x)\n",
        "model_inference = tf.keras.Model(input_string,new_output)"
      ],
      "metadata": {
        "id": "oaKj4LpDU366"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simuler des données de texte et faites l'évaluation du modele dessus\n",
        "test_sentences = tf.constant([\"I always taught that the sky was red\",\"there is no limit about what accomplish in life\"])"
      ],
      "metadata": {
        "id": "WKPVMrq-VDGk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inference(test_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR-ONy40TGhA",
        "outputId": "6ab31ec8-9abe-407a-df79-15bfa7a6deb2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
              "array([[0.5074701],\n",
              "       [0.5039931]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# TASK 2"
      ],
      "metadata": {
        "id": "n_xHpBVLWIx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simuler données\n",
        "dico_sentences = tf.constant([\"Tu sais, j'aime tellement le sex\",\n",
        "                              \"J'ai toujours eu envie de te dire que tu étais une réelle merde\",\n",
        "                              \"Francisco, il est temps que je t'avoue quelque chose, je suis ton père\"])\n",
        "\n",
        "# Instancier vectorizer ngrams\n",
        "ngram_vectorizer = tf.keras.layers.TextVectorization(output_mode='multi_hot',ngrams = 2)\n",
        "\n",
        "# Faire l'adaption\n",
        "ngram_vectorizer.adapt(dico_sentences)\n",
        "\n",
        "# Tester avec et sans multi hot\n",
        "ngram_vectorizer(\"Tu sais, la vie est belle petite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5hltR2XWKhF",
        "outputId": "edcbdec5-c18c-468d-b30e-4de1d62a746f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fafd17a85f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(59,), dtype=float32, numpy=\n",
              "array([1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Designer un petit modele en API fonctionnelle : \n",
        "# juste un modele dense apres la couche input multi hot\n",
        "input_layer = tf.keras.layers.Input(shape=(ngram_vectorizer.vocabulary_size(),),dtype = \"float32\")\n",
        "x = tf.keras.layers.Dense(1,activation=\"sigmoid\")(input_layer)\n",
        "model = tf.keras.Model(input_layer,x)"
      ],
      "metadata": {
        "id": "XB5Fn9cmWqTG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simuler un dataset de training \n",
        "training_sentences = [\"Il y a ce truc que j'ai jamais osé t'avouer\",\n",
        " \"ce qui compte dans la vie, c'est pas ce que tu fais, c'est ce que tu montres au monde\"]\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((training_sentences,[1,0]))\n",
        "training_dataset = training_dataset.map(lambda a,b : (ngram_vectorizer(a),b))\n",
        "training_dataset = training_dataset.batch(2)\n",
        "\n",
        "# trainer le modele, compile et fit\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy())\n",
        "model.fit(training_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs-yyrcuXu8U",
        "outputId": "7bfe498e-42ca-4ba0-b6d0-4e244d9baf75"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 404ms/step - loss: 0.5249\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fafcc687f10>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simuler des données d'inférence\n",
        "test_sentences = tf.constant([\"Ca me rappelle la fois où ta mère m'a demandé si j'étais homo\",\n",
        "                              \"Chousno, voudrais tu faire un tour au lac avec Daddy Yankee?\",\n",
        "                              \"Tu penseras à prendre ton sac poubelle avec toi\",\n",
        "                              \"Il faudra bien nettoyer les lieux quand on s'en ira\"])\n",
        "\n",
        "# Créer un modèle d'inférence en rajoutant une input layer recevant une string en entrée\n",
        "input_string = tf.keras.layers.Input(shape=(1,),dtype=\"string\")\n",
        "x = ngram_vectorizer(input_string)\n",
        "output = model(x)\n",
        "model_inference = tf.keras.Model(input_string,output)\n",
        "\n",
        "# Evaluer ces données\n",
        "model_inference(test_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fx6VkYcYH_y",
        "outputId": "fb921e05-9304-4117-89cf-f87a82131b9e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
              "array([[0.5442313 ],\n",
              "       [0.5443399 ],\n",
              "       [0.53121364],\n",
              "       [0.5966401 ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# TASK 3 "
      ],
      "metadata": {
        "id": "wR6vYDKtaqjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pareil que les deux dernieres taches\n",
        "# Mais utilisez un tfidf multihot\n",
        "# Modele pareil que la tache 2"
      ],
      "metadata": {
        "id": "r-ShRPk5asYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simuler données\n",
        "dico_sentences = tf.constant([\"Tu sais, j'aime tellement le sex\",\n",
        "                              \"J'ai toujours eu envie de te dire que tu étais une réelle merde\",\n",
        "                              \"Francisco, il est temps que je t'avoue quelque chose, je suis ton père\"])\n",
        "\n",
        "# Instancier vectorizer ngrams\n",
        "ngram_vectorizer = tf.keras.layers.TextVectorization(output_mode='tf-idf',ngrams = 2,)\n",
        "\n",
        "# Faire l'adaption\n",
        "ngram_vectorizer.adapt(dico_sentences)\n",
        "\n",
        "# Tester avec et sans multi hot\n",
        "ngram_vectorizer(\"Je te dois quelque chose Francisco, je ne suis pas ton père mais ton frère\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItcKwW7cUvEB",
        "outputId": "cf6896bb-fbf3-4a15-812d-1cbe686c6ba3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(59,), dtype=float32, numpy=\n",
              "array([15.446135  ,  0.        ,  0.        ,  1.8325815 ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.91629076,  1.8325815 ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.91629076,\n",
              "        0.        ,  0.        ,  0.        ,  0.91629076,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.91629076,\n",
              "        0.91629076,  0.        ,  0.        ,  0.91629076,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.91629076,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.91629076], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# A SAVOIR\n"
      ],
      "metadata": {
        "id": "0GXcWPLfJA4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A savoir\n",
        "# layers.TextVectorization() afin de vectoriser du texte direct sans prise de tête, il se charge de tout le preprocessing\n",
        "# text_vectorizer.adapt() afin d'initialiser le vectorizer du text\n",
        "# Pour faire du ngram tokenizer : layers.TextVectorization(output_mode=\"multi_hot\", ngrams=2) # checker pr voir pkoi multi_hot\n",
        "# Emnedding layer a un input dim la taille du vocabulaire et un output dim la taille de l'espace latent\n",
        "# Acceder a la taille du vocabulaire d'un vectorizer : vectorizer.vocabulary_size()\n",
        "# Accéder au vocabulaire d'un vectorizer : vectorizer.get_vocabulary()\n",
        "# Dans un mapping sur un dataset tensorflow, la fonction lambda doit prendre deux arguments et non un seul quand bien même celui ci serait un tuple\n",
        "# Multi hot dans un vectorizer de text, c'est juste que ca te renvoie un one hot encoding plutot qu'un integer c 'est tout"
      ],
      "metadata": {
        "id": "YMzTUPL6JEs2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}