{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hf_utilisation_04.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkTV30GvjxCIpTE/u0j2Cw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hansglick/book_errata/blob/main/hf_utilisation_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQPZX9uBUjMH"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "gsu8JCGeUqpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " * url : https://huggingface.co/course/fr/chapter2/5?fw=tf\n",
        " * video : https://www.youtube.com/watch?v=ROxrFOEbsQE"
      ],
      "metadata": {
        "id": "W3YxVQTJZqb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lots d'observations (batch)"
      ],
      "metadata": {
        "id": "6u_HhjhSVKeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les modèles de Hugging Face ne prennent en entrée que des tensors. Ceux-ci doivent être batchés à savoir que la première dimension du batch doit être reservé pour spécifier le nombre d'observations.\n",
        "\n",
        "```python\n",
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
        "print(tokenized_inputs[\"input_ids\"])\n",
        "# Une dimension a bien été rajouté lors de l'application du tokenizer afin de bien livrer un batch au modele\n",
        "```"
      ],
      "metadata": {
        "id": "ZJXYj3D1VPb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding, en détails\n",
        "\n",
        "Il est nécessaire de padder les observations, car les tensors sont des objects *rectangulaires*. Et on le répète, les inputs des modèles sont des tensors. Ainsi on padd, i.e. on rajoute autant de tokens que nécessaire (le plus souvent à droite) afin d'obtenir des phrases de même longueur que la phrase la plus longue du jeu de données. L'identifiant du token de padd est accessible ici `tokenizer.pad_token_id` Afin d'avertir le modèle que les poids liés aux tokens de padd ne sont pas à prendre en compte lors du calcul de la loss, on doit également fourinir un masque d'attention qui spécifie quels sont les tokens à prendre en compte et ceux à ne pas prendre en compte. Le masque d'attention doit lui aussi être de la forme d'un batch évidemment\n",
        "\n",
        "```python\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))\n",
        "print(outputs.logits)\n",
        "```\n",
        "\n",
        "```python\n",
        "tf.Tensor(\n",
        "[[ 1.5693681  -1.3894582 ]\n",
        " [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)\n",
        "```\n",
        "\n",
        "En utilisant l'argument padding = True, tout se fait en transparence. Je reste sur ma faim car il explique pas pourquoi on n'a pas besoin de padder jusqu'à la taille limite sur lequel le transformer a été trained\n",
        "\n",
        "```python\n",
        "tokenizer(sentences,padding=True))\n",
        "```"
      ],
      "metadata": {
        "id": "OEeR2VWwa3-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taille limite des séquences\n",
        "\n",
        "Les transformeurs ont des longueurs limite définies. Si on leur donne en input des longueurs qui excedent la taille limite, ils planteront. 2 options s'offrent à nous pour traiter les longues séquences :\n",
        " * les tronquer, de sorte à ce qu'elles ne dépassent jamais la taille limite\n",
        " * utiliser des transformers adequates comme le LongFormer ou le LED voir https://huggingface.co/docs/transformers/model_doc/longformer et https://huggingface.co/docs/transformers/model_doc/led"
      ],
      "metadata": {
        "id": "Am2qomFjchUL"
      }
    }
  ]
}