{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p022_other_tokenizers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwhYLx4p4ORAgHbLDy3gxB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hansglick/book_errata/blob/main/p022_other_tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXdIrkdmQcmY",
        "outputId": "6757923e-ffa2-47d4-97e6-ab721f806e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q \"tensorflow-text==2.8.*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ymLuQcfSBsl",
        "outputId": "5083680d-f84f-4f36-e0fa-a09cffaacb4c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ],
      "metadata": {
        "id": "-SiDqtarSHIk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Les différents tokenizers de Tensorflow\n",
        "\n",
        "Les inputs strings doivent être encodés en UTF-8 pour être compatibles avec les tokenizers présentés ci-dessus"
      ],
      "metadata": {
        "id": "k6iWHaOAQjAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        " *  `text.WhitespaceTokenizer()`"
      ],
      "metadata": {
        "id": "sN-FE7eaQvmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est clairement le tokenizer le plus naïf, il split une phrase à chaque white space (comme les espaces, les tabulations, les sauts de ligne, etc). Le problème c'est qu'il ne distingue pas la ponctuation ce con. Par exemple : `J'aime aller au cinéma, regarder du bon vieux film américain` sera tokenisé de la façon suivante : J'aime + aller + au + cinéma, + regarder + ... Il a crée un token `cinéma,`. Il a mis une \",\" le con. "
      ],
      "metadata": {
        "id": "hNTel1AxQ3OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYkx8kliR4sh",
        "outputId": "3e65f522-acfa-4f5c-f102-234721aaa2d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'What', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____\n",
        " * `UnicodeScriptTokenizer()`"
      ],
      "metadata": {
        "id": "YGGsKnC9SMcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour parer au problème lié à la ponctuation du whitespacetokenizer() vu plus haut, il existe le `UnicodeScriptTokenizer()`. Le défaut de ce tokenizer c'est qu'il splittera le mot `can't` en trois mots `can` + `'` + `t`, ce qui n'est nécessairement pas ce qu'on souhaite."
      ],
      "metadata": {
        "id": "tzfxGjxUSbWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R3UqiW-TN_k",
        "outputId": "05ef2644-51dc-4035-b128-e7c76bab7455"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'What', b'you', b'know', b'you', b'can', b\"'\", b't', b'explain', b',', b'but', b'you', b'feel', b'it', b'.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        " * `tf_text.BertTokenizer()`"
      ],
      "metadata": {
        "id": "TgZxXLwDTPBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est l'implémentation du tokenizer utilisé dans le papier présentant BERT. En gros, il s'appuie sur le WordPiece algorithm mais rajoute quelques trucs en plus comme la lowerisation par exemple. Le tokenizer BERT nécessite l'accès au corpus dans un premier temps afin de constituer le vocabulaire"
      ],
      "metadata": {
        "id": "uARORCieUmI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_wp_en_vocab.txt?raw=true\"\n",
        "r = requests.get(url)\n",
        "filepath = \"vocab.txt\"\n",
        "open(filepath, 'wb').write(r.content)\n",
        "\n",
        "tokenizer = tf_text.BertTokenizer(filepath, token_out_type=tf.string, lower_case=True)\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS3S7tfcVcao",
        "outputId": "4d9e21cb-42e7-40d8-9f07-8a7635f8db1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52382"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____\n",
        " * `tf_text.SentencepieceTokenizer()`"
      ],
      "metadata": {
        "id": "XdEjW7DcV5Vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le même genre que BERT tokenizer on a le SentencepieceTokenizer, il ne rentre pas dans les détails donc je m'emmerde pas trop"
      ],
      "metadata": {
        "id": "IFreqW9aWK6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_oss_model.model?raw=true\"\n",
        "sp_model = requests.get(url).content\n",
        "\n",
        "tokenizer = tf_text.SentencepieceTokenizer(sp_model, out_type=tf.string)\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "id": "6jXhrWtbVAVM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        " * `tf_text.UnicodeCharTokenizer()`"
      ],
      "metadata": {
        "id": "F38m5W30V4B0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans les langues CJK, qui ne contiennent pas d'espaces, il est intéressant de tokeniser tout les caractères UTF-8. C'est ce que permet de faire `tf_text.UnicodeCharTokenizer()`. Le token de sortie est un nombre représentant un nombre représentant un caractère utf-8. On peut donc le décoder facilement avec la fonction `tf.strings.unicode_encode(tf.expand_dims(tokens, -1), \"UTF-8\")` pour obtenir un ragged tensor des caractères décodés."
      ],
      "metadata": {
        "id": "kt8loK-BXONA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.UnicodeCharTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())\n",
        "\n",
        "tokens = tokenizer.tokenize([\"M O A\"])\n",
        "print(tokens.to_list())\n",
        "\n",
        "characters = tf.strings.unicode_encode(tf.expand_dims(tokens, -1), \"UTF-8\")\n",
        "print(characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xQsM6NXYLVt",
        "outputId": "b82116e7-5910-44b6-c029-8f64502d3906"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[87, 104, 97, 116, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 121, 111, 117, 32, 99, 97, 110, 39, 116, 32, 101, 120, 112, 108, 97, 105, 110, 44, 32, 98, 117, 116, 32, 121, 111, 117, 32, 102, 101, 101, 108, 32, 105, 116, 46]]\n",
            "[[77, 32, 79, 32, 65]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        " * `tf_text.HubModuleTokenizer()`"
      ],
      "metadata": {
        "id": "Q9MdJo82ZghE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il existe des tokenizers spécifiques présents sur le Hub de Tensorflow. Notamment celui qui permet de parser du chinois, i.e. de séparer les mots entre eux alors qu'il n'existe pas d'espaces dans cette langue. Souvent, ce qu'on fait, c'est qu'on crée des fonctions de décodage et encodage UTF-8 pour lire convenablement les caractères"
      ],
      "metadata": {
        "id": "hqbwUWF7ZpxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_HANDLE = \"https://tfhub.dev/google/zh_segmentation/1\"\n",
        "segmenter = tf_text.HubModuleTokenizer(MODEL_HANDLE)\n",
        "tokens = segmenter.tokenize([\"新华社北京\"])\n",
        "print(tokens.to_list())\n",
        "\n",
        "# Un poil énervé la récursivité\n",
        "def decode_list(x):\n",
        "  if type(x) is list:\n",
        "    return list(map(decode_list, x))\n",
        "  return x.decode(\"UTF-8\")\n",
        "def decode_utf8_tensor(x):\n",
        "  return list(map(decode_list, x.to_list()))\n",
        "\n",
        "print(decode_utf8_tensor(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvW8d_jPaY-s",
        "outputId": "d7ca0f44-dfc7-4e79-b9ba-8d6d13514177"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'\\xe6\\x96\\xb0\\xe5\\x8d\\x8e\\xe7\\xa4\\xbe', b'\\xe5\\x8c\\x97\\xe4\\xba\\xac']]\n",
            "[['新华社', '北京']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        " * `tf_text.SplitMergeTokenizer()`"
      ],
      "metadata": {
        "id": "9ol-TP2laxF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut vouloir tokenizer un corpus en connaissant l'exact position des tokens. A ce moment là, SplitMergeTokenizer est un excellent choix. Il prend en input en plus des strings, un tableau de label qui indique le début d'un nouveau mot, représenté par 0. Le 1 indique que le caractère fait partie du mot dernièrement commencé. On peut également utilisé `tf_text.SplitMergeFromLogitsTokenizer()` qui prendrait des paires de logits issues d'un neural network pour qualifier le début ou pas de chaque mot mais j'ai rien compris."
      ],
      "metadata": {
        "id": "nhHg38Ribi8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strings = [\"新华社北京\"]\n",
        "labels = [[0, 1, 1, 0, 1]]\n",
        "tokenizer = tf_text.SplitMergeTokenizer()\n",
        "tokens = tokenizer.tokenize(strings, labels)\n",
        "print(decode_utf8_tensor(tokens))\n",
        "\n",
        "strings = [[\"新华社北京\"]]\n",
        "labels = [[[5.0, -3.2], [0.2, 12.0], [0.0, 11.0], [2.2, -1.0], [-3.0, 3.0]]]\n",
        "tokenizer = tf_text.SplitMergeFromLogitsTokenizer()\n",
        "tokenizer.tokenize(strings, labels)\n",
        "print(decode_utf8_tensor(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u_55589csKt",
        "outputId": "b184344d-27db-4bae-b4ff-1c9c2d7ff9d3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['新华社', '北京']]\n",
            "[['新华社', '北京']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____\n",
        " * `tf_text.RegexSplitter()`"
      ],
      "metadata": {
        "id": "dBBUrQsEcvT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permet de splitter une phrase à chaque regex pattern extraits de la phrase en question. "
      ],
      "metadata": {
        "id": "8DB8J6bKc9q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = tf_text.RegexSplitter(\"\\s\")\n",
        "tokens = splitter.split([\"What you know you can't explain, but you feel it.\"], )\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDYArvxkdI1M",
        "outputId": "3a026b46-771c-4bcf-9ffe-c259384f25d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'What', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____\n",
        "____\n",
        "### Offsets methods\n",
        "Tout les tokenizers sont capables de renvoyer les positions de départ et de fin de chaque token sur la phrase originale"
      ],
      "metadata": {
        "id": "DDVyelQqdLBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(['Everything not saved will be lost.'])\n",
        "print(tokens.to_list())\n",
        "print(start_offsets.to_list())\n",
        "print(end_offsets.to_list())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idESXZo0dmSY",
        "outputId": "8110733f-2294-455d-fdfb-8a0779236a54"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'Everything', b'not', b'saved', b'will', b'be', b'lost', b'.']]\n",
            "[[0, 11, 15, 21, 26, 29, 33]]\n",
            "[[10, 14, 20, 25, 28, 33, 34]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____\n",
        "### Detokenization methods\n",
        "Tout les tokenizers proposent une méthode de detokenization afin de passer du tensor de tokens au tensor de text tokens"
      ],
      "metadata": {
        "id": "uT8ZJUVRdu2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.UnicodeCharTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())\n",
        "strings = tokenizer.detokenize(tokens)\n",
        "print(strings.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx-vx7TgeBmH",
        "outputId": "88f4b508-0c6e-4b68-85e0-09e8db25c9fe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[87, 104, 97, 116, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 121, 111, 117, 32, 99, 97, 110, 39, 116, 32, 101, 120, 112, 108, 97, 105, 110, 44, 32, 98, 117, 116, 32, 121, 111, 117, 32, 102, 101, 101, 108, 32, 105, 116, 46]]\n",
            "[b\"What you know you can't explain, but you feel it.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____\n",
        "### Interaction avec les Tensorflow Datasets\n",
        "La tokenization des strings présents dans un tensorflow dataset se fait naturellement à l'aide la méthode `.map()` comme dans l'exemple ci-dessous :"
      ],
      "metadata": {
        "id": "GPhumEspeNNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
        "iterator = iter(tokenized_docs)\n",
        "print(next(iterator).to_list())\n",
        "print(next(iterator).to_list())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoI3NnoXeprx",
        "outputId": "73e47562-7f7e-4543-bf38-53e6be217991"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
            "[[b\"It's\", b'a', b'trap!']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HVlXUQCJe6cM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}