{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p021_WordPiece_Algorithm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNBDd7FfoTVoN7wgluKIl0W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hansglick/book_errata/blob/main/p021_WordPiece_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNMXwyja8LAO",
        "outputId": "1b0350d7-01e2-4eff-a0e3-eab6d45f9e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo3yDIh5BAWs",
        "outputId": "82cfdfbf-2bac-40b1-bdb1-3a113567b2f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordPiece Algorithm\n",
        "\n",
        "____\n",
        "\n",
        "### Overview\n",
        "\n",
        " * Etant donné D taille du vocabulaire, trouvez les D word pieces tel que pour un corpus représentatif, on observe un nombre de tokens minimum\n",
        " * Il y aurait deux grands algorithms : \n",
        "   * Le bottom-Up\n",
        "   * Le Top-Down, celui qui est implémenté dans TensorFlow\n",
        " * Concernant les langues comme le chinois ou le sud coréen, WordPiece n'est pas adéquat, on lui préfère d'autres modèles comme par exemple : https://tfhub.dev/google/zh_segmentation/1\n",
        "\n",
        "____\n",
        "\n",
        "### Fonction de base de WordPiece\n",
        "\n",
        " * *input* : \n",
        "   * Un dictionnaire D de comptage (mot:fréquence)\n",
        "   * Un threshold T\n",
        " *  *output* : \n",
        "   * Un Vocabulaire V\n",
        "\n",
        "____\n",
        "\n",
        "### Une itération de l'algorithme\n",
        " 1. Compter la fréquence des sous-séquences \"éligibles\"\n",
        " 2. Trier les sous-séquences de la sous-séquence la plus longue à la moins longue\n",
        " 3. Itérer sur les sous-séquences\n",
        " 4. Si la fréquence de la sous-séquence est supérieure à un threshold T, alors on garde la sous-séquence et on décompte de la fréquence des autres sous-séquences enfants de la fréquence de la sous-séquence sélectionnée. Par exemple on arrive au mot `incapable` de fréquence `44`. Il est sélectionné car sa fréquence est supérieure au threshold 21. On doit donc alors retranché de `44` toutes les fréquences des sous-séquences émanant de `incapable`. Par exemple `able` est une sous séquence de fréquence `147`. 44 parmi 147 provient du mot capable. De ce fait, la fréquence de `able` est réevaluée à `147-44 = 103`\n",
        " 5. Lorsque toutes les sous séquences ont été sélectionnées, alors on vient de finir une itération\n",
        "\n",
        "____\n",
        "\n",
        "### Eligibilité des sous-séquences\n",
        "\n",
        " * Lors de la première itération sont éligible **toutes** les sous-séquences qui composent un mot. Par exemple : le mot `chat` contient les sous séquences suivantes : \n",
        "   * c ch cha chat\n",
        "   * h ha hat\n",
        "   * a at\n",
        "   * t\n",
        " * Lors des itérations suivantes, sont éligibles les sous séquences qui commencent par un segmentation point **après** avoir run le WordPiece pour en extraire les tokens. Par exemple on run le WordPiece et le mot unbeatable est décomposé en `un` + `beat` + `able`. A ce moment les sous séquences éligibles sont les suivantes :     \n",
        "   * u un unb unbe unbea unbeat unbeata unbeatab unbeatabl unbeatable\n",
        "   * b be bea beat beata beatab beatabl beatable\n",
        "   * a ab abl able\n",
        "\n",
        "____\n",
        "\n",
        "### Nombre d'itérations\n",
        "\n",
        "En pratique on run 4 ou 5 itérations"
      ],
      "metadata": {
        "id": "GYSiv8Iq8Ujh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A faire sur un corpus wikipedia"
      ],
      "metadata": {
        "id": "1ahOyuK11NJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"gdrive/MyDrive/data/wikipedisr.txt\"\n",
        "\n",
        "with open(path,encoding=\"utf8\", errors='ignore') as file:\n",
        "    for line in file:\n",
        "        print(line.rstrip())\n",
        "        break"
      ],
      "metadata": {
        "id": "3O2_ou5qD6DB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}